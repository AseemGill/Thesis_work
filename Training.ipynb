{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9a424c",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552b3b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\Anaconda3\\envs\\Aseem\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn.modules import activation\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b500a55",
   "metadata": {},
   "source": [
    "### Get Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a56063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty the gpu cache and check if the gpu is available for use\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2bfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTION FOR SETTING UP DATA LOADERS\n",
    "def prepare_dataset(data_dir, batch_size =  64, num_workers = 0, flag = None):\n",
    "    '''\n",
    "    Prepares dataloaders for training. Datasets are prepare with a three-cross \n",
    "    fold validation approach. \n",
    "    \n",
    "    data_dir - Path to directory containing samples from one rat's training set\n",
    "               this directory will contain sub-directories which are folders of \n",
    "               each class (Dorisflexion, plantarflexion, and pricking of the heel)\n",
    "    batch_size - how many signals are loaded per batch in the data loaders\n",
    "    num_workers - number of processes loading batches in parallel\n",
    "    \n",
    "    Types\n",
    "    data_dir - string\n",
    "    batch_size - int\n",
    "    num_workers - int\n",
    "    '''\n",
    "    # The Test and Validation sets are taken from a single fold\n",
    "    if flag == \"Test\" or flag == \"Val\":\n",
    "        # Specify where the folder containing the validation or testing signals are for the dataset\n",
    "        data = torchvision.datasets.DatasetFolder(data_dir, loader = torch.load, extensions = \".pt\")\n",
    "\n",
    "        # Prepare data loaders\n",
    "        # Signals are loaded in batch sizes as specified \n",
    "        loader = torch.utils.data.DataLoader(data, batch_size=batch_size, \n",
    "                                                num_workers=num_workers, shuffle=True)\n",
    "    \n",
    "    # The Training set is the combination of the two remaining folds\n",
    "    \n",
    "    else:\n",
    "        # Specify where the folders containing the training signals are for the dataset\n",
    "        data1 = torchvision.datasets.DatasetFolder(data_dir[0], loader = torch.load, extensions = \".pt\")\n",
    "        data2 = torchvision.datasets.DatasetFolder(data_dir[1], loader = torch.load, extensions = \".pt\")\n",
    "        \n",
    "        # Concatenate the datasets\n",
    "        train_data = torch.utils.data.ConcatDataset((data1,data2))\n",
    "\n",
    "        # Prepare data loaders\n",
    "        # Signals are loaded in batch sizes as specified \n",
    "        loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                                num_workers=num_workers, shuffle=True)\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3819d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_fold_cross_sets(base_dir,fold1_dir,fold2_dir,fold3_dir,test_dir, batch_size =  64, num_workers = 0):\n",
    "    '''\n",
    "    # Loads the datasets in a three cross fold validation method\n",
    "    base_dir - directory containing the rats data and signal class\n",
    "    fold1_dir,fold2_dir,fold3_dir,test_dir - directories of the fold 1,2,3 and testing fold\n",
    "    batch_size - how many signals are loaded per batch in the data loaders\n",
    "    num_workers - number of processes loading batches in parallel\n",
    "    \n",
    "    Types\n",
    "    base_dir,fold1_dir,fold2_dir,fold3_dir,test_dir - string\n",
    "    batch_size - int\n",
    "    num_workers - int\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Fold 1\n",
    "    train_set_1 = prepare_dataset([fold1_dir,fold2_dir], batch_size, num_workers)\n",
    "    valid_set_1 = prepare_dataset(fold3_dir, batch_size, num_workers, \"Val\")\n",
    "\n",
    "    #Fold 2\n",
    "\n",
    "    train_set_2 = prepare_dataset([fold1_dir,fold3_dir], batch_size, num_workers)\n",
    "    valid_set_2 = prepare_dataset(fold2_dir, batch_size, num_workers, \"Val\")\n",
    "\n",
    "    #Fold 3\n",
    "    train_set_3 = prepare_dataset([fold2_dir,fold3_dir],batch_size, num_workers)\n",
    "    valid_set_3 = prepare_dataset(fold1_dir, batch_size, num_workers, \"Val\")\n",
    "\n",
    "    test_set = prepare_dataset(test_dir, batch_size, num_workers, \"Test\")\n",
    "    \n",
    "    return train_set_1, valid_set_1, train_set_2, valid_set_2, train_set_3, valid_set_3, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77fc2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    '''\n",
    "    Class for early stopping criteria, which will stop training if \n",
    "    validation loss stops increasing is is increasing very slowly\n",
    "    \n",
    "    min_delta - The minimum change in loss that can increase tolerance\n",
    "    Tolerance - Number of epochs that decrease or (small increase) \n",
    "                in loss occurs for. \n",
    "    '''\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, prev_val_loss, curr_val_loss):\n",
    "        \n",
    "        # Check if loss increases more than min_delta\n",
    "        if (curr_val_loss - prev_val_loss) >= self.min_delta:\n",
    "            self.counter +=1 # If loss increases, increase counter by one\n",
    "            \n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True # If counter equals or exceeds tolerance stop training\n",
    "        \n",
    "        # Rest counter to 0  if loss decreases less than min_delta\n",
    "        else:\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3dc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, Trainset, Validset, num_classes, classes, learning_rate=0.01, num_epochs=30, early_stop =5, use_cuda = True, min_delta = 0, min_epochs = 30, title = None):\n",
    "    '''\n",
    "    net - network to be trained\n",
    "    Trainset - Training data\n",
    "    Validation - Data\n",
    "    num_classes - number of potential output classes\n",
    "    classes - possible class outputs\n",
    "    learning_rate - learning rate for optimizer\n",
    "    num_epochs - number of epochs to train for \n",
    "    early_stop - number of epochs for early stopping criterion\n",
    "    use_cuda - leverage gpu for training\n",
    "    min_delta - amount of change in validation loss required to trigger early stop\n",
    "    title - title of network\n",
    "    \n",
    "    Types\n",
    "    net - Pytorch Neural Network Object\n",
    "    Trainset, Validset - Pytorch Dataloader Object\n",
    "    num_classes - int\n",
    "    classes - list of strings\n",
    "    learning_rate - float\n",
    "    num_epochs - int\n",
    "    early_stop - int\n",
    "    use_cude - bool\n",
    "    min_delta - float\n",
    "    title - string or None\n",
    "    '''\n",
    "    ########################################################################\n",
    "    # Fixed PyTorch random seed for reproducible result\n",
    "    torch.manual_seed(1000)\n",
    "    ########################################################################\n",
    "    # Define the Loss function and optimizer\n",
    "    # The loss function will be Binary Cross Entropy (BCE). In this case we\n",
    "    # will use the BCEWithLogitsLoss which takes unnormalized output from\n",
    "    # the neural network and scalar label.\n",
    "    # Optimizer will be SGD with Momentum.\n",
    "    criterion = nn.CrossEntropyLoss() # Using Cross Entropy Loss\n",
    "#     criterion = nn.NLLLoss() # Using Cross Entropy Loss\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.8)\n",
    "    ########################################################################\n",
    "    # Move model to gpu if available\n",
    "    device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    ########################################################################\n",
    "    # Set up some numpy arrays to store the training/test loss/accuracy\n",
    "    train_acc = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    ########################################################################\n",
    "    # Train the network\n",
    "    # Loop over the data iterator and sample a new batch of training data\n",
    "    # Get the output from the network, and optimize our loss function.\n",
    "    start_time = time.time()\n",
    "    early_stopping = EarlyStopping(tolerance=early_stop, min_delta = min_delta)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):  # loop over the dataset multiple times\n",
    "        total_train_loss = 0.0\n",
    "        total_train_acc = 0.0\n",
    "        total_epoch = 0\n",
    "        print(\"EPOCH: \" + (str(epoch + 1)))\n",
    "        for i, data in tqdm(enumerate(Trainset, 0)):\n",
    "            # Turn on training mode, *turns on batch normalization and dropout\n",
    "            net.train() #*****************************#\n",
    "\n",
    "            # Get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "            # Check if gpu is available and flagged for use\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.double()\n",
    "                inputs = inputs.cuda() # copy image batch to gpu memory\n",
    "                labels = labels.cuda() # copy label batch to gpu memory\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass, backward pass, and optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "#             outputs = net(inputs)\n",
    "#             output1 = outputs[0]\n",
    "#             output2 = outputs[1]\n",
    "#             loss1 = criterion(output1, labels)\n",
    "#             loss2 = criterion(output2, labels)\n",
    "            \n",
    "#             outputs = output1 \n",
    "# #             print(outputs)\n",
    "#             loss = loss1 + loss2*0.3\n",
    "            \n",
    "#             loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate the statistics\n",
    "            guess = torch.argmax(outputs, axis=1)\n",
    "            corr = (guess).squeeze() == labels\n",
    "            corr = sum(corr)\n",
    "            total_train_acc += corr\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)        \n",
    "\n",
    "\n",
    "        train_acc[epoch] = (float(total_train_acc)) / total_epoch\n",
    "        train_loss[epoch] = float(total_train_loss) / (i+1)\n",
    "        val_acc[epoch], val_loss[epoch] = evaluate(net, criterion, Validset, use_cuda = True)\n",
    "        if epoch >= min_epochs and val_acc[epoch] > 0.85:\n",
    "            early_stopping(val_loss[epoch-1], val_loss[epoch])\n",
    "        \n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            break\n",
    "        print((\"Epoch {}: Train Acc: {}, Train loss: {} |\"+\n",
    "                \"Validation Acc: {}, Validation loss: {}\").format(\n",
    "                epoch + 1,\n",
    "                train_acc[epoch],\n",
    "                train_loss[epoch],\n",
    "                val_acc[epoch],\n",
    "                val_loss[epoch]))\n",
    "\n",
    "        # Save the current model (checkpoint) to a file every 5 epochs\n",
    "        if val_acc[epoch] > 0.8:\n",
    "            torch.save(net.state_dict(), \"model_checkpoints/\" + title + \"_epochno_\" + str(epoch))\n",
    "            print('Finished Training')\n",
    "            end_time = time.time()\n",
    "\n",
    "    # SAVE FINAL MODEL\n",
    "    end_time = time.time()\n",
    "    \n",
    "    train_acc = train_acc[:epoch]\n",
    "    train_loss = train_loss[:epoch]\n",
    "    val_acc = val_acc[:epoch]\n",
    "    val_loss = val_loss[:epoch]\n",
    "    \n",
    "    if not os.path.isdir(\"model_checkpoints/\" + rat + \"//\"):\n",
    "        os.mkdir(\"model_checkpoints/\" + rat + \"//\")\n",
    "    torch.save(net.state_dict(), \"model_checkpoints/\" + title + \"_final\")\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "    # Write the train/val loss/acc into CSV file for plotting later\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    np.savetxt(\"{}_train_acc.csv\".format(\"model_checkpoints//\" + title), train_acc)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(\"model_checkpoints//\" + title), train_loss)\n",
    "    np.savetxt(\"{}_val_acc.csv\".format(\"model_checkpoints//\"+ title), val_acc)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(\"model_checkpoints//\" + title), val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993c918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, loader, use_cuda = True):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "    Args:\n",
    "     net: PyTorch neural network object\n",
    "     criterion: The loss function\n",
    "     loader: PyTorch data loader for the validation set\n",
    "     use_cuda: Bool selecting whether to evaluate on the GPU or not\n",
    "    Returns:\n",
    "     acc: A scalar for the avg classification acc over the validation set\n",
    "     loss: A scalar for the average loss function over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    ########################################################################\n",
    "    # Move model to gpu if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    net = net.cuda()\n",
    "\n",
    "    # Initialize acc, loss, and epoch_no\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_epoch = 0.0\n",
    "\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        # Turn on Evaluation mode, *turns off batch normalization and dropout\n",
    "        net.eval() #*****************************#\n",
    "\n",
    "        # Takes data from train loader and splits into labels and inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Check if GPU is available and selected\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            inputs = inputs.cuda() # Returns of copy of the image batch to the GPU memory\n",
    "            labels = labels.cuda() # Returns of copy of the labels  to the GPU memory\n",
    "\n",
    "        # Forward pass\n",
    "        \n",
    "        outputs = net(inputs.double())\n",
    "#         outputs = outputs[0]\n",
    "        loss = criterion(outputs, labels)\n",
    "#         outputs = outputs[0]\n",
    "\n",
    "        # Calculate rate of incorrect predictions\n",
    "        guess = torch.argmax(outputs, axis=1)\n",
    "        corr = (guess).squeeze() == labels\n",
    "        corr = sum(corr)\n",
    "\n",
    "        # Update acc, loss, epoch number\n",
    "        total_acc += corr\n",
    "        total_loss += loss.item()\n",
    "        total_epoch += len(labels)\n",
    "\n",
    "    # Compute acc and loss\n",
    "    acc = float(total_acc) / total_epoch\n",
    "    loss = float(total_loss) / (i + 1)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa3a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODEL #### **Can change as desired\n",
    "\n",
    "class MSCB(nn.Module):\n",
    "    def __init__(self, small_kernel, medium_kernel, large_kernel, num_filters):\n",
    "        super(MSCB, self).__init__()\n",
    "        self.name = \"MSCB\"\n",
    "\n",
    "        # Define Small Path\n",
    "        self.convS = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = small_kernel, padding = 'same')\n",
    "        self.MPoolS = nn.MaxPool1d(kernel_size = small_kernel, stride = 5, padding = int(small_kernel/2 - 1))\n",
    "        \n",
    "        # Define Medium Path\n",
    "        self.convM = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = medium_kernel, padding = 'same')\n",
    "        self.MPoolM = nn.MaxPool1d(kernel_size = medium_kernel, stride = 5, padding = int(medium_kernel/2 - 1))\n",
    "        \n",
    "        # Define Large Path\n",
    "        self.convL = nn.Conv1d(in_channels = 56, out_channels = num_filters, kernel_size = large_kernel, padding = 'same')\n",
    "        self.MPoolL = nn.MaxPool1d(kernel_size = large_kernel, stride = 5, padding = int(large_kernel/2 - 1))\n",
    "        \n",
    "        # MPool first\n",
    "        self.MPool = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.conv = nn.Conv1d(in_channels = 56, out_channels = 128, kernel_size = 24, padding = 'same')\n",
    "        \n",
    "        # Post Concatenation\n",
    "        self.conv2 = nn.Conv1d(in_channels = 3*num_filters + 128, out_channels = 64, kernel_size = 112, padding = 'same')\n",
    "        self.MPool2 = nn.MaxPool1d(kernel_size = 3, stride = 5)\n",
    "        self.fc1 = nn.Linear(in_features = 3840, out_features = 400)\n",
    "        self.Dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features = 400, out_features = 1024)\n",
    "        self.fc3 = nn.Linear(in_features = 1024, out_features = 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Feature Learning Head\n",
    "        \n",
    "        # Reshape Tensor\n",
    "        x = torch.moveaxis(x,2,1)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        # Parrallel Convolution Pathways\n",
    "        x_S = self.MPoolS(F.relu(self.convS(x)))\n",
    "        x_M = self.MPoolM(F.relu(self.convM(x)))\n",
    "        x_L = self.MPoolL(F.relu(self.convL(x)))\n",
    "        x_O = F.relu(self.conv(self.MPool(x)))\n",
    "        \n",
    "        # Post Concatenation\n",
    "        x = torch.cat((x_S,x_M,x_L,x_O),1)\n",
    "        x = self.MPool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flattening\n",
    "        x = x.view(-1,3840)\n",
    "\n",
    "        #Classification Head\n",
    "        x = F.relu(self.fc1((x)))\n",
    "        x = self.Dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0275d093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cf_mat_gen(net, test_set):\n",
    "    '''\n",
    "    Generates Confusion Matrix\n",
    "    \n",
    "    net - neural network\n",
    "    test_set - test set\n",
    "    \n",
    "    Types\n",
    "    net - Pytorch Neural Network Object\n",
    "    test_set - pytorch dataloader object\n",
    "    '''\n",
    "    guesses = []\n",
    "    labels = []\n",
    "    \n",
    "    # Evaluate model for generating confusion matrix\n",
    "    for i in test_set: # Iterate through batches\n",
    "        batch = (i[0])\n",
    "        num,__,_ = batch.shape\n",
    "\n",
    "        label_batch = (i[1])\n",
    "        for j in range(num): # Evaluate each sample in a batch individually\n",
    "\n",
    "            label = np.array(label_batch)[j] # Obtain class label\n",
    "            sample = batch[j,:,:].unsqueeze(0) # Obtain signal\n",
    "            sample = sample.cuda() # move signal to gpu\n",
    "            \n",
    "            \n",
    "            net.eval() # set network to evaluation model (no dropout or tuning of normalization)\n",
    "            net = net.cuda() # move network to gpu\n",
    "            \n",
    "            # Evaluate\n",
    "            outputs = (net(sample))\n",
    "            outputs = outputs[0]\n",
    "            outputs = torch.Tensor.cpu(outputs)\n",
    "            np_out = outputs.detach().numpy()[0]\n",
    "            guess = np.argmax(np_out, axis=0)\n",
    "            guesses.append(guess)\n",
    "            labels.append(label)\n",
    "            \n",
    "    # Generate Confusion Matrix\n",
    "    cf_mat = confusion_matrix(guesses, labels)\n",
    "    \n",
    "    return cf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606242fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(array):\n",
    "    # Computes Accuracy\n",
    "    total_samples = np.sum(array)/1.\n",
    "    correct = np.sum(np.multiply(array,np.eye(3)))\n",
    "\n",
    "    accuracy = round(correct/total_samples*100,2)\n",
    "    return(accuracy)\n",
    "def recall(array, index):\n",
    "    # Computes Recall\n",
    "    num = array[index,index]\n",
    "    den = np.sum(array[index,:])\n",
    "    \n",
    "    return(num/den)\n",
    "\n",
    "def precision(array, index):\n",
    "    # Computes Precision\n",
    "    num = array[index,index]\n",
    "    den = np.sum(array[:,index])\n",
    "    \n",
    "    return(num/den)\n",
    "\n",
    "def macro_f1(array):\n",
    "    # Computes Macro F1 score\n",
    "    recall_arr = []\n",
    "    precision_arr = []\n",
    "\n",
    "    for i in range(3):\n",
    "        recall_arr.append(recall(array, i))\n",
    "        precision_arr.append(precision(array, i))\n",
    "        \n",
    "    per_class_f1 = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        per_class_f1.append((2*recall_arr[i]*precision_arr[i])/(recall_arr[i] + precision_arr[i]))\n",
    "        \n",
    "    f1 = sum(per_class_f1)/3\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0edf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for Training\n",
    "num_filters = 32\n",
    "learning_rate = 0.005\n",
    "num_epochs = 300\n",
    "use_cuda = True\n",
    "early_stop = 3\n",
    "batch_size = 16\n",
    "kernel_set = [[1, 3, 5]]\n",
    "min_delta = -0.025\n",
    "min_epochs = 3\n",
    "num_workers = 8\n",
    "# hidden_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa1b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rat 2\n",
      "Starting Fold 1\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010970354080200195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 300,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bd082c8dd84dc2bab7777eca0cab3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011967182159423828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ae3e51fe0446f68a04aa5bbd80dd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GillA\\AppData\\Local\\Temp\\ipykernel_7720\\40127382.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "C:\\Users\\GillA\\Anaconda3\\envs\\Aseem\\lib\\site-packages\\torch\\nn\\modules\\conv.py:303: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Convolution.cpp:883.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Acc: 0.3325428194993412, Train loss: 1.0990371344966834 |Validation Acc: 0.3333333333333333, Validation loss: 1.0991266984283254\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011967658996582031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdd017591b94d4789373cb2b31736e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Acc: 0.3324110671936759, Train loss: 1.0990060903550534 |Validation Acc: 0.3333333333333333, Validation loss: 1.0991296609131138\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01196742057800293,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1189869942504adaa4abdc2f8ced5670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Acc: 0.33179622310057094, Train loss: 1.0991764861378777 |Validation Acc: 0.3333333333333333, Validation loss: 1.0991185741143281\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018948078155517578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75562ae2cc594796856cc0f84855286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = 1\n",
    "for kernels in kernel_set: # Iterate through each potential kernel set\n",
    "    small_kernel, medium_kernel, large_kernel = kernels\n",
    "    set_no = \"\\nKernel set \" + str(count) + \"\\n\"\n",
    "    \n",
    "    for i in [2,3,4,5,6,7,8,9,10]: # Iterate through each rat      \n",
    "\n",
    "        \n",
    "        rat = \"Rat \" + str(i)\n",
    "        if not os.path.isdir(\"model_checkpoints//\" + rat + \"//\"): # Make folder to save results\n",
    "            os.mkdir(\"model_checkpoints//\" + rat + \"//\")\n",
    "        \n",
    "        # Generate a base title for the model\n",
    "        base_title = rat + \"//\" + rat + (\"kernels_{0}_{1}_{2}_\").format(small_kernel,medium_kernel,large_kernel)\n",
    "\n",
    "        print(rat)\n",
    "        \n",
    "        # Specify directory of data\n",
    "        base_dir = \"D:\\Aseem\\Spike Firing Rate 1500\\Augmented_Dataset\\\\\" + rat + \"\\\\\"\n",
    "#         base_dir = \"D:\\Aseem\\Spike Firing Rate 1500\\Minimum Dataset\\\\\" + rat + \"\\\\\"\n",
    "        \n",
    "        # Specify directory of each fold\n",
    "        fold1_dir = base_dir + \"Fold1\"\n",
    "        fold2_dir = base_dir + \"Fold2\"\n",
    "        fold3_dir = base_dir + \"Fold3\"\n",
    "        test_dir = base_dir + \"Test\"\n",
    "\n",
    "        # Load datasets\n",
    "        train_set_1, valid_set_1, train_set_2, valid_set_2, train_set_3, valid_set_3, test_set = three_fold_cross_sets(base_dir,fold1_dir,fold2_dir,fold3_dir,test_dir, batch_size, num_workers)\n",
    "\n",
    "        # Train Fold 1\n",
    "        fold = rat + \"--\" + \"Fold1\"\n",
    "        print(\"Starting Fold 1\")\n",
    "        title = base_title + \"Fold_1\"\n",
    "        one_CNN = MSCB(small_kernel, medium_kernel, large_kernel, num_filters)\n",
    "        train_network(one_CNN, train_set_1, valid_set_1, 3, [\"DF\", \"PF\", \"Prick\"], learning_rate, num_epochs, early_stop, use_cuda, min_delta, min_epochs, title)\n",
    "        \n",
    "        # Generate confusion matrices\n",
    "        cf_mat_train1 = cf_mat_gen(one_CNN, train_set_1)\n",
    "        cf_mat_val1 = cf_mat_gen(one_CNN, valid_set_1)\n",
    "\n",
    "        torch.cuda.empty_cache() # Empty GPU Cache\n",
    "        \n",
    "        # Train Fold 2\n",
    "        fold = \"Fold2\"\n",
    "        print(\"Starting Fold 2\")\n",
    "        title = base_title + \"Fold_2\"\n",
    "        one_CNN = MSCB(small_kernel, medium_kernel, large_kernel, num_filters)\n",
    "        train_network(one_CNN, train_set_2, valid_set_2, 3, [\"DF\", \"PF\", \"Prick\"], learning_rate, num_epochs, early_stop, use_cuda, min_delta, min_epochs, title)\n",
    "        \n",
    "        # Generate confusion matrices\n",
    "        cf_mat_train2 = cf_mat_gen(one_CNN, train_set_2)\n",
    "        cf_mat_val2 = cf_mat_gen(one_CNN, valid_set_2)\n",
    "        \n",
    "        torch.cuda.empty_cache() # Empty GPU Cache\n",
    "        \n",
    "        # Train Fold 3\n",
    "        fold = \"Fold3\"\n",
    "        print(\"Starting Fold 3\")\n",
    "        title = base_title + \"Fold_3\"\n",
    "        one_CNN = MSCB(small_kernel, medium_kernel, large_kernel, num_filters)\n",
    "        train_network(one_CNN, train_set_3, valid_set_3, 3, [\"DF\", \"PF\", \"Prick\"], learning_rate, num_epochs, early_stop, use_cuda, min_delta, min_epochs, title)\n",
    "        \n",
    "        # Generate confusion matrices\n",
    "        cf_mat_train3 = cf_mat_gen(one_CNN, train_set_3)\n",
    "        cf_mat_val3 = cf_mat_gen(one_CNN, valid_set_3)\n",
    "\n",
    "        \n",
    "        # Write Training Summary File\n",
    "        f = open(\"model_checkpoints//\" + rat + \"//\" + rat + (\"kernels_{0}_{1}_{2}\").format(small_kernel,medium_kernel,large_kernel) + \"_summary.txt\", \"a\")\n",
    "        f.write(set_no)\n",
    "        f.write((\"\\nSmall Kernel = {}\\nMedium Kernel = {}\\nLarge Kernel = {}\\nNum Filters = {}\\nLearning Rate = {}\\nPatience = {}\\nBatch Size = {}\\nMin Delta = {}\\nMin Epochs = {}\\n\").format(small_kernel,\n",
    "                                                                                                                                   medium_kernel,\n",
    "                                                                                                                                   large_kernel,\n",
    "                                                                                                                                   num_filters,\n",
    "                                                                                                                                   learning_rate,\n",
    "                                                                                                                                   early_stop,\n",
    "                                                                                                                                   batch_size,\n",
    "                                                                                                                                   min_delta,\n",
    "                                                                                                                                   min_epochs))\n",
    "        f.write(\"\\nFold_1\\n\")\n",
    "        f.write(\"Training\\n\")\n",
    "        f.write(str(cf_mat_train1))\n",
    "        f.write(\"\\nTraining Accuracy: \" + str(accuracy(cf_mat_train1)) + \"%\\n\")\n",
    "        f.write(\"Training Macro F1 Score: \" + str(macro_f1(cf_mat_train1)))\n",
    "        f.write(\"\\nValidation\\n\")\n",
    "        f.write(str(cf_mat_val1))\n",
    "        f.write(\"\\nValidation Accuracy: \" + str(accuracy(cf_mat_val1)) + \"%\\n\")\n",
    "        f.write(\"Validation Macro F1 Score: \" + str(macro_f1(cf_mat_val1)))\n",
    "        \n",
    "        \n",
    "        f.write(\"\\n\\nFold_2\\n\")\n",
    "        f.write(\"Training\\n\")\n",
    "        f.write(str(cf_mat_train2))\n",
    "        f.write(\"\\nTraining Accuracy: \" + str(accuracy(cf_mat_train2)) + \"%\\n\")\n",
    "        f.write(\"Training Macro F1 Score: \" + str(macro_f1(cf_mat_train2)))\n",
    "        f.write(\"\\nValidation\\n\")\n",
    "        f.write(str(cf_mat_val2))\n",
    "        f.write(\"\\nValidation Accuracy: \" + str(accuracy(cf_mat_val2)) + \"%\\n\")\n",
    "        f.write(\"Validation Macro F1 Score: \" + str(macro_f1(cf_mat_val2)))\n",
    "        \n",
    "        f.write(\"\\n\\nFold_3\\n\")\n",
    "        f.write(\"Training\\n\")\n",
    "        f.write(str(cf_mat_train3))\n",
    "        f.write(\"\\nnTraining Accuracy: \" + str(accuracy(cf_mat_train3)) + \"%\\n\")\n",
    "        f.write(\"Training Macro F1 Score: \" + str(macro_f1(cf_mat_train3)))\n",
    "        f.write(\"\\nValidation\\n\")\n",
    "        f.write(str(cf_mat_val3))\n",
    "        f.write(\"\\nValidation Accuracy: \" + str(accuracy(cf_mat_val3)) + \"%\\n\")\n",
    "        f.write(\"Validation Macro F1 Score: \" + str(macro_f1(cf_mat_val3)))\n",
    "        f.close()\n",
    "        torch.cuda.empty_cache() # Empty GPU Cache\n",
    "        \n",
    "    count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
